# Example .cycling-ai.yaml Configuration File
# Copy this to your home directory or project root as .cycling-ai.yaml

# Prompt version to use (determines which prompt templates are loaded)
version: "1.3"

# LLM Provider Configuration
# Uncomment and configure your preferred provider(s)

providers:
  # AWS Bedrock - Enterprise-grade with pay-per-use pricing
  bedrock:
    model: anthropic.claude-3-5-sonnet-20241022-v2:0
    # Optional: Specify AWS region (defaults to us-east-1)
    # region: us-east-1
    # Optional: Use named AWS profile from ~/.aws/credentials
    # profile_name: my-aws-profile
    # Optional: AWS Bedrock Guardrails (content filtering, PII detection)
    # NOTE: Guardrails are NOT compatible with tool use
    # guardrail_id: your-guardrail-id
    # guardrail_version: DRAFT
    # guardrail_trace: false

  # Anthropic Claude - Direct API access (recommended for development)
  anthropic:
    model: claude-3-5-sonnet-20241022
    # API key can be set here or via ANTHROPIC_API_KEY environment variable
    # api_key: sk-ant-...

  # OpenAI GPT - Direct API access
  openai:
    model: gpt-4-turbo
    # API key can be set here or via OPENAI_API_KEY environment variable
    # api_key: sk-...

  # Google Gemini - Best value option
  gemini:
    model: gemini-1.5-pro
    # API key can be set here or via GOOGLE_API_KEY environment variable
    # api_key: ...

  # Ollama - Local execution, no API key needed
  ollama:
    model: llama3.1:8b
    # Optional: Custom Ollama host
    # host: http://localhost:11434

# RAG (Retrieval Augmented Generation) Configuration
rag:
  # Project vectorstore location (shared knowledge base)
  project_vectorstore: data/vectorstore

  # User vectorstore location (athlete-specific history)
  user_vectorstore: ~/.cycling-ai/athlete_history

  # Embedding provider: "local" or "openai"
  embedding_provider: local

  # Embedding model (when using local)
  # embedding_model: all-MiniLM-L6-v2

# Logging Configuration
logging:
  # Log level: DEBUG, INFO, WARNING, ERROR
  level: INFO

  # Log file location
  file: ~/.cycling-ai/logs/cycling-ai.log

  # LLM interaction logs (for debugging)
  interactions_dir: ~/.cycling-ai/logs/llm_interactions

  # Enable verbose console logging
  verbose: false

# Session Configuration
sessions:
  # Session storage directory
  storage_dir: ~/.cycling-ai/sessions

  # Maximum session age in days (for cleanup)
  max_age_days: 30

# Report Generation Defaults
reports:
  # Default output directory
  output_dir: ./reports

  # Default analysis period in months
  period_months: 6

  # Default training plan duration in weeks
  training_plan_weeks: 12

  # Workout source: "library" (fast, deterministic) or "llm" (flexible, uses tokens)
  workout_source: library

# Model-specific prompt customization
# Override default prompts with model-specific versions
# prompt_model: default  # Options: default, gemini, custom

# Custom prompts directory (optional)
# If set, loads prompts from this directory instead of defaults
# prompts_dir: ~/.cycling-ai/custom_prompts

---

# Common Configuration Examples

## Example 1: AWS Bedrock with Specific Region and Profile
# Best for: Production deployment on AWS infrastructure

# version: "1.3"
# providers:
#   bedrock:
#     model: anthropic.claude-3-5-sonnet-20241022-v2:0
#     region: us-west-2
#     profile_name: production

## Example 2: Cost-Optimized with Claude Haiku
# Best for: High-volume processing, development

# version: "1.3"
# providers:
#   bedrock:
#     model: anthropic.claude-3-haiku-20240307-v1:0
#   anthropic:
#     model: claude-3-haiku-20240307

## Example 3: Multi-Provider Setup
# Best for: Flexibility, fallback options

# version: "1.3"
# providers:
#   bedrock:
#     model: anthropic.claude-3-5-sonnet-20241022-v2:0
#   anthropic:
#     model: claude-3-5-sonnet-20241022
#   gemini:
#     model: gemini-1.5-pro
# # Use --provider flag to switch between them

## Example 4: Local Development with Ollama
# Best for: Privacy, offline development, cost-free

# version: "1.3"
# providers:
#   ollama:
#     model: llama3.1:8b
# # Requires Ollama running: ollama serve

## Example 5: Enterprise AWS with Custom Configuration
# Best for: Large organizations with compliance requirements

# version: "1.3"
# providers:
#   bedrock:
#     model: anthropic.claude-3-5-sonnet-20241022-v2:0
#     region: us-east-1
#     profile_name: cycling-ai-prod
# logging:
#   level: INFO
#   file: /var/log/cycling-ai/application.log
#   interactions_dir: /var/log/cycling-ai/interactions
# sessions:
#   storage_dir: /var/lib/cycling-ai/sessions
#   max_age_days: 90
# reports:
#   output_dir: /mnt/shared/cycling-reports
#   period_months: 6

---

# Usage Notes:

# 1. File Location Options:
#    - Project root: .cycling-ai.yaml (highest priority)
#    - User home: ~/.cycling-ai/.cycling-ai.yaml
#    - Global: /etc/cycling-ai/config.yaml (lowest priority)

# 2. Environment Variables Override Config File:
#    - ANTHROPIC_API_KEY
#    - OPENAI_API_KEY
#    - GOOGLE_API_KEY
#    - AWS_ACCESS_KEY_ID / AWS_SECRET_ACCESS_KEY (for Bedrock)

# 3. CLI Arguments Override Everything:
#    cycling-ai generate --provider bedrock --model <model-id>

# 4. Sensitive Data:
#    - Prefer environment variables for API keys
#    - Use AWS profiles/IAM roles instead of hardcoding credentials
#    - Never commit .cycling-ai.yaml with secrets to version control

# 5. Testing Configuration:
#    cycling-ai config show  # Display current configuration
#    cycling-ai config validate  # Validate configuration file

---

# Security Best Practices:

# ✅ DO:
# - Use environment variables for API keys
# - Use AWS profiles or IAM roles for Bedrock
# - Set restrictive file permissions: chmod 600 .cycling-ai.yaml
# - Add .cycling-ai.yaml to .gitignore
# - Use separate configs for dev/staging/production

# ❌ DON'T:
# - Commit API keys to version control
# - Share credentials in config files
# - Use root/admin accounts for API access
# - Hardcode AWS credentials in config

---

# For more information:
# - AWS Bedrock Setup: docs/AWS_BEDROCK_USER_GUIDE.md
# - Full Documentation: README.md
# - Troubleshooting: docs/TROUBLESHOOTING.md
